
# 1. Preprocessing raw data of different sensors

SIP supports the preprocessing of raw data to extract enhanced images and features as input to the classification algorithms. Parameters are illustrated using landsat8 as an example.  

```
landsat8_params:
    landsat8_zip_dir: /home/l44xu/SIP2/data/landsat8_raw_zip
    raw_landsat8_format: tar.gz
    to_unzip: False
    multilook_num: 1
    low_percentage_to_remove: 0.02
    high_percentage_to_remove: 0.02
    output_raw_img_dir: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/
```

**1.1 Unzip the raw data.** If ***to_unzip*** is True, the raw data files with extension ***raw_landsat8_format*** in folder ***landsat8_zip_dir*** will be extracted into separate folders. 

**1.2 Multilooking to reduce size.** To reduce image size, a image patch of size ***multilook_num*** by ***multilook_num*** will be averaged into a single pixel in the preprocessed images. 

**1.3 Remove extrame pixels.** To increase image contrast for better visualization, low intensity pixels of percentage ***low_percentage_to_remove*** will be set to the value 0, and high intensity pixels of percentage ***high_percentage_to_remove*** will be set to the value 255. After this, the dynamic range of image will be extended. 

**1.4 Save preprocessed images.** The preprocessed images will be saved into folder ***output_raw_img_dir***.
 
# 2. Preparing data 

SIP automatically prepares the data for classification. No manual operatoins are needed. 

```
raw_data_params:
    raw_img_dir: /home/l44xu/SIP/data/landsat8_preprocessed_imgs
    raw_img_type: tiff
    raw_id_str_range: #from 'start' to 'end' in the name of the raw image are the string that differenciate different scenes
        start: 10
        end: 34
    raw_band_names: # each raw scene contrain the following bands, each of which is a seperate image in 'raw_img_type' format
        - B2
        - B3
        - B4
```

**2.1 Define the folder of your images** In ***raw_img_dir***, you have multiple scenes, each scene may have multiple bands/channels covering the same location/target. All bands/channels have the format of ***raw_img_type***.

**2.2 How does SIP discriminate different scenes.** To tell SIP about this, you need to make sure that the file name of different scenes within range ***raw_id_str_range*** are different. For example, if ***start*** is 1 and ***end*** is 8, then 20200101_B2.tiff and 20200505_B3.tiff are two different scenes. But, 20200101_B2.tiff and 20200101_B3.tiff are of the same scene. 

**2.3 How does SIP know which bands/channels you want to use.** To tell SIP about this, you need to specify ***raw_band_names*** with different ID names, and make sure your images contain these ID names. In the above code, for each scene, three images whose file name contrain respectively ***B2***, ***B3***, and ***B4*** will be used. 
   

# 3. Copy the prepared data to different folders

SIP automatically generates folders and copies data to separate folders for different data processing stages, i.e., ***train***, ***validation***, ***test*** and ***prediction*** stages. 

* **train** folders contain ***data***, ***mask*** and ***result*** for training the classifier.

* **val** folders contain ***data***, ***mask*** and ***result*** for validating the classifier during training or model selection. 

* **test** folders contain ***data***, ***mask*** and ***result*** for testing the accuracies of the classifiers. 

* **prediction** folders contain ***data***, ***mask*** and ***result*** for predicting new images once the model is finalized. ***prediction*** images do not have ground truth masks, but ***test*** images must have ground truth masks to estimate accuracies. 

For each stage, i.e., train, validation, test and prediction, there are ***data***, ***mask*** and ***save*** folders. 

* **data** folders contain prepared bands/channels for each scene. The file format of each scene is defined by ***image_type***. Each scene has a file in the data folder. 

* **mask** folders contain ground truth mask files. The file format is defined in ***mask_type***. Now SIP only supports the ***png*** format.

* **save** folders contain all results generated by SIP classifiers. 

```
dirs:
    image_type: npz
    mask_type: png

    # A (image, mask) pair is identified by looking for the same file name in the 'data' subfolder and 'mask' subfolder
    # E.g., 'hsi1.npz' in /data/train/ is associated with 'hsi1.npz' in /mask/train/ folder
    # ----------------------------------------------------------------
    # (1) config image directories
    
    data: 
    
        # (1.1) training_data_dir contains images with GT in training_mask_dir for training
        train: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/data/train
        
        # (1.2) val_data_dir contains images with GT in val_data_dir for validation
        val: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/data/val
        
        # (1.3) test_data_dir contains images with GT in test_mask_dir for test to acquire training accuracies
        test: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/data/test
        
        # (1.4) predict_data_dir contains images that do not have GT for prediction to acquire classification maps
        predict: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/data/predict
        
    #-----------------------------------------------------------------
    # (2) config GT directories
    
    mask:   
     
        # (2.1) training_mask_dir contrains one-to-one matching masks with images in training_data_dir
        train: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/mask/train
        
        # (2.2) val_mask_dir contains one-to-one matching masks with images in val_data_dir
        val: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/mask/val
        
        # (2.3) test_mask_dir contains one-to-one matching masks with images in test_data_dir
        test: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/mask/test

        # (2.4) predict_mask_dir contains the background pixel masks for some images in predict_data_dir
        predict: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/mask/predict
        
    #------------------------------------------------------------------
    # (3) config results directories
    
    save:
        
        # (3.1) specify directory to save model i.e. best model log file
        model: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/save/model
        
        # (3.2) specify directory to save training results i.e. maps accuracies
        train: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/save/train
        
        # (3.3) specify directory to save val results i.e. maps accuracies
        val: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/save/val
        
        # (3.4) specify directory to save test results i.e. maps accuracies
        test: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/save/test
        
        # (3.5) specify directory to save predict results i.e. maps
        predict: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/save/predict
 
``` 

# 4. Define classes and colors for classification maps
 
SIP allows the user to specify the number of classes, class names and colors. 

```
classification_map_params:

    num_classes: 2
    
    class_color_config_params:
        my_colors:
            - white # color of the background, e.g., land area in sea ice classification
            - yellow # color of the first class
            - indigo # color of the second class
        my_classes: # the class names you defined here have to be consistent with the class names appeared in GUI
                    # e.g., in GUI, if you have land_train, land_val, land_test, water_train, water_val, water_test, 
                    # then, here you need to have two classes, i.e., land and water. 
            - background # 0, background mask has to end with 'background_mask.xxx' 
            - burn # 1, the first class name
            - unburn # 2, the second class name

    #define the indicator of the background class whose class identity is unknown
    #the color of the background class is defined as the first color in 'my_colors'
    background_class_indicator: 0
```

**4.1 Each class has a color.** The length of ***my_colors*** should be equal to the length of ***my_classes***, which are equals to ***num_classes*** + 1, because the presence of the ***background*** class.  

**4.2 Background class.** The ***background*** class consists of the pixels that are not relevant, and usually are not used in the classification. For example, the land pixels in ocean classification. Land masks are needed to mask out the land area, which is defined here as the background class. 

**4.3 Class names should be exactly the same with SIP GUI**. For example, if under ***my_classes***, there are two classes, i.e., 'aaa' and 'bbb'. Then, in SIP GUI, when you draw ground truth, you have to define 'aaa_train', 'bbb_train', or optionally, 'aaa_val', 'bbb_val' for validation, and 'aaa_test', 'bbb_test' for testing. 


# 5. Generate samples for training classifiers

There are two types of samples, i.e., patch samples and image samples. 

* **patch samples** are ***small image patches***, e.g., 5-by-5, or 11-by-11 small images, which are used as input to determine the class label of the center pixels in image patchs. The use of an image patch centered at the referenced pixel rather than a single pixel enables the utilization of local spatial information when determining the label of the referenced pixel. Patch samples are usually used in classic convolutional neural network (CNN) models whose input is an image patch and output is the class memebership of the centered pixel. 

* **image samples** are ***big sub-images***, e.g., 512-by-512, which are used as input to determine the class label of all pixels in the sub-image. Image samples are usually used in fully convolutional neural network (FCN) whose input is a sub-image and output is a label map involving the labels of all pixels in the sub-image. 

```
data_params: 


    img_data_params: #image input to FCN models
        split_size: 145
        overlap: 5

    patch_data_params: #patch input to CNN models with fully connected layers
        patch_size: 1  # **

        save_extracted_patches: False #whether the extracted patch samples will be saved

        is_dataset_cuda: False # 'True' means all patches deployed on cuda; 'False' means cuda will be used batch-wise; 
        extract_patch_cuda: False # 'True' means performing F.unfold() on cuda()

        to_extract_patches_blockwise: True
        extract_patches_block_size_r: 1000
        extract_patches_block_size_c: 1000

        to_predict_patches_blockwise: True
        predict_patches_block_size_r: 1000
        predict_patches_block_size_c: 1000
 
```

**5.1 patch samples** are defined by ***patch_data_params***. If ***save_extracted_patches*** is True, all patch samples will be saved into hard disk, so that next time they will be automatically loaded. But, if you have limited hard disk space, you may want to set it to False. 

**5.2 use GPU.** If ***extract_patches_cuda*** is True, GPU will be used to extract patches. If ***is_dataset_cuda*** is True, all extracted patches will be deployed onto GPU. If you have limited GPU memeory or too many patches, you may want to set it to False, and then only one batch will be deployed onto GPU when training, instead of all batches.

**5.3 blcokwise computation.** If ***to_extract_patches_blockwise***, patch samples will be extracted in a block wise manner, meaning that when extracting patches, SIP will load and scan each block instead of the whole image. You may want to use this preference, if you have large image or limited memory. Similarly, if ***to_predict_patches_block_wise*** is True, SIP will load and predict each block instead of the whole image in case of large image or limited memory resource. 

**5.4 image samples** are defined by ***img_data_params***, where ***split_size*** defines the size of the sub-images and ***overlap*** defines the overlapping width between two adjacent sub-images.    

# 6. Training parameters

Here, it defines all the parameters related to classifier training.

```
train_params: 
    lr: 0.0001
    epoch: 50  
    batch_size_train: 2000 # batch_size when doing training
    batch_size_val: 20000 # batch_size when doing val and test
    val_inter: 1 # if 1, then validation and test are used in each epoch 
    to_test: False # is True, both validation and test data will be used to evaluate training
    prop_test: 1 # if 0.1, then 10% of all test samples are used in each epoch
    prop_train: 0.0001 # if 0.1, then 10% of all train samples are used in each epoch
    var: 0.05 # std of noise added to input when using images as input 
    net_type: knn
 
    train_mask_save_options:
        save_train_mask: False
        img_format: png
    
    input_img_save_options:
        save_input_img: False # if True, original images will be saved
        img_format: png
        is_rgb: False # whether want to display rgb image, if not, all channels will be saved separately
        rgb_bands: 0_0_0 # specify r, g, and b channels
        vmin: 0 # vmin of gray image, [vmin, vmax] is the range that the colormap covers
        vmax: 255 # vmax of gray image, [vmin, vmax] is the range that the colormap covers

    map_save_options:
        save_classification_map: True
        img_format: png
        copy_map_to_raw_data_dir: True
        save_map_over_epoch: True
        use_background_pixels: True


train_result_params:
    train_log_file: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/save/model/ss_res_batchSize2000_epoch50_lr0.0001_time20200613223826_train.log
    model_file: ss_res_batchSize2000_bestEpoch50_lr0.0001_time20200613225304_weights.pkl
    train_config_file: /home/l44xu/SIP/data/landsat8_preprocessed_imgs/all_data/save/model/ss_res_batchSize2000_bestEpoch50_lr0.0001_time20200613225304__time20200613225305_train_config.yaml

``` 

**6.1 which classifier to use** is defined by ***net_type***. 

**6.2 some parameters are only relevant to neural network models.** ***epoch*** defines how many iterations of trying out all training samples. ***batch_size_train*** and ***batch_size_val*** are respectively the batch sizes of the training and validation operation. ***val_inter*** defines how many epochs have to be finished before performing one validation of the classifier. ***var*** defines the standard deviation of the noise added to the input data when using fully convolutional neural network (FCN) classifiers. 

**6.3 use a subset of training or test samples.** If you do not want to use all the training or test samples, you can use ***prop_train*** to specify the percentage of training samples randomly picked up in training, and ***prop_test*** for the percentage of random test samples to use. 

**6.4 save training mask.** If you want to  

Intelligent pixel-level image classification using deep neural networks.

![](./pics/vhr.png)

# Features

* **Strong feature learning capability** of deep neural networks to capture weak signature information for accurate pixel-level classification and mapping.

* **Fast processing and mapping** of a large number of image scenes by leverating GPU computation;

* **Quickly drawing some examplar pixels** using lines, points and polygons on one scene gives you high-precision pixel-level label map of the whole scene;

* Quickly drawing some lines, points and polygons on mutiple scenes gives you **a robust classifer that generates accurate predictions on new images**; 

* Accurate pixel-level classification and mapping of **many hundreds or thousands large scenes** (e.g., 5k by 5k pixels) can be achived within a single day using **a single desktop with GPU**;

* **Safe and convenient** for research, industry or governmental usage where all the data and data processing is local, avoiding the troubles of uploading your data to the cloud.

* Runs on **multiple system settings**, e.g., loptop with/without GPU, desktop with/without GPU, server with/without GPU; 

* Runs on **multiple operational systems**, e.g., Linux, Windows, MacOS; 

* **Strong preprocessing functionalities** to support open source or commercial images, e.g., **Drone hyperspectral/multispectral image**, Sentinel-1/2/3, RADARSAT-1/2, **RADARSAT Constellation Mission (RCM)**, Landsat, MODIS, Drone images, and other **biomedical and industrial images**;

# Documentation
* [Config file] (docs/config_file.md)
* [Burned area detection from Landsat8 Images] (docs/ba_landsat8.md)
<!---* [Getting started](docs/get-started.md)--->
<!---* [Introduction](intro.md)--->
<!---* [Parameters](parameters.md)--->
<!---* [How To](how-to.md)--->
<!---* [FAQ](faq.md)--->
<!---* [Related Websites](related-website.md)--->


## Installation guide:

**Step 1: Install anaconda.** Install conda for python 3.7: https://docs.anaconda.com/anaconda/install/linux/. For Winows, please go to https://www.anaconda.com/distribution/. 

**Step 2: Create python 3.8.2 environment.** Create a conda environment 'py38' that runs on python version 3.8.2. 
```
conda create -n py38 python=3.8
```

**Step 3: Activate 'py38':**

```
conda activate py38
```

**Step 4: Install the following packages:**
```
conda install pytorch 
conda install pandas
conda install -c anaconda pyqt
conda install -c anaconda scikit-image
conda install -c anaconda scikit-learn
conda install pyyaml
conda install git
```
For Windows, the above applies except pytorch. Please use 
```
conda install pytorch torchvision cudatoolkit=10.1 -c pytorch
```

**Step 5: Cd to your home folder, and run**
```
cd you_home_folder
git clone https://github.com/spectrumAI/SIP.git
```

cd to the folder 'SIP' to run the app:
```
cd SIP
python SIP.py
```

**Step 6: Get a license to run.** Please email spectrumaitech@gmail.com with your name and organization to get a free license. Put the license.lic under the pytransform folder and replace the old one. 

# Process Sentinel-1 SAR image for sea ice classification

![](./pics/classify.gif)

**Step 1: Run app and open data.** 
- Run SIP;
- Open the '.json' file in ***'SIP/data/sentinel1_preprocessed_imgs'***;
- Also open the associated '.tiff' image.

**Step 2: Draw region of interst (ROI).**  
- ***Double click a class*** in the 'Label List' panel on the right to choose a class; 
- Draw point, or line or polygon to add more ROI for this class;
- ***To finish drawing line and polygon, type 'c' from keyboard***;
- Save drawing using default name;

**Step 3: Prepare label mask.** 
- Click on ***"prepare label mask"***, and then specify the csv file you just saved;
- This step transfer ROIs from vectors to images;
- Take a look at the png images generated in the "Image List" panel on the left;

**Step 4: Edit config file.** 
- Open the ***"sentinel1_config_os.yaml"*** config file in the 'config' folder;
- **Make sure you have changed all dirs to your own directories**.

**Step 5: Prepare all dirs and data.** 
- Click on ***"prepare all dirs and data"*** to prepare all training, test and prediction data. 
- You need to choose the .yaml 'config' file you just edited. 
- Take a look at all the folders generated and the 'npz' files. 

**Step 6: Train classifier.** 
- Click on ***'train classifier'*** and then choose the .yaml config file you just edited. 
- Once training is finished, you can see the generated label map by clicking on this file in the 'Image List' panel on the left. 
- Check ***the training and validation accuracies*** in the "train_log" file under the "save_model" folder specified in the .yaml config file you edited. 
- Change the ***number of epoches*** in the .yaml config file and see what happens. 

**Step 7: Test classifier.** 
- You can optionally run ***"Test classifier"***, but it will run on the same image in this example. 
- You also need to select the same .yaml config file. 
- Check the ***test accuracies*** in the "test_log" file under the "save_model" folder specified in the .yaml config file.  

**Step 8: Predict label map on a new image.** 
- Click on ***"Predict image"*** to run the trained model on the other scene in the folder.
- Once it is done, you can also check the label map in the "Image List" panel. You also need to select the same .yaml config file.
- Check the "predict_log" file under the "save_model" folder specified in the .yaml config file. 


# Preprocess raw Sentinel-1 data by just one click:

**Step 1: Download data.** Downlaod Sentinel1 SAR scenes from https://search.asf.alaska.edu/#/.

**Step 2: Copy all downloaded .zip files to /data/sentinel1_raw_zip/ folder.** 

**Step 3: Preprocess.** 
- Run SIP, and click on ***"preprocessing -> sentinel1"***;
- First select the /data/sentinel1_raw_zip/ folder, and then ***select the sentinel1_config_os.yaml file***;
- Take a look at the preprocessed scenes in /data/sentinel1_preprocessed_imgs/ folder;
- Change the 'multilook_num' parameter in the .yaml config file and re-run the program to see what happens. 

**Step 4: Classification.** Go to step 1 in the sentinel-1 example and start from there to finish.

# Preprocess raw Landsat8 L1TP data. 

**Step 1: Download data.** Downlaod Landsat8 L1TP scenes from USGS Earthexplorer https://earthexplorer.usgs.gov. 

**Step 2: Copy all downloaded .tar.gz files to /data/landsat8_raw_zip/ folder.** 

**Step 3: Preprocess.** 
- Run SIP, and click on ***"preprocessing -> Landsat8 L1TP"***;
- First select the /data/landsat8_raw_zip/ folder, and then ***select the landsat8_config_os.yaml file***;
- Take a look at the preprocessed scenes in /data/landsat8_preprocessed_imgs/ folder;
- Change the 'multilook_num' parameter in the .yaml config file and re-run the program to see what happens. 

**Step 4: Classification.** Go to step 1 in the sentinel-1 example and start from there to finish.



# Process RCM SLC CP Winnipeg scene for land cover classification

**Step 1: Download data.** Downlaod Winnipeg RCM SLC CP scene from ftp://ftp.asc-csa.gc.ca/users/OpenData_DonneesOuvertes/pub/RCM/. Copy the data to /data/rcm_raw_data/ folder.

**Step 2: Edit config file** Open config/rcm_slc_cp_config.yaml, and change all dirs to your own dirs.

**Step 3: Open file.** Run SIP, and click on "preprocessing" manu, and click on "RCM SLC CP". ** You need to first select the .safe file in RCM SLC CP data folder, and then select the .yaml file you just edited.

**Step 4: Classification.** Go to step 3 in the sentinel-1 example and start from there to finish.

![](./pics/rcm.png)

![](./pics/rcm_result.png)

